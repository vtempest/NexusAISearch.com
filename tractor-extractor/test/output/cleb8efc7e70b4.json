{
  "url": "https://medium.com/the-artificial-impostor/use-textrank-to-extract-most-important-sentences-in-article-b8efc7e70b4",
  "title": "Use TextRank to Extract Most Important Sentences in Article",
  "author": "Ceshine Lee",
  "date": "2019-04-18",
  "source": "Medium",
  "html": "Photo CreditI&#x2019;m trying to build a NLP system that can automatically highlight the important part of an article to help people to read long articles. The common practice is to start with a simple baseline model that is useful enough, and then incrementally improves the performance. The TextRank algorithm[1], which I also used as a baseline in a text summarization system, is a natural fit to this task.Available ImplementationsThere are multiple open-sourced Python implementations of TextRank algorithm, including ceteri/pytextrankdavidadamojr/TextRanksummanlp/textrankI need some ways to bypass the text pre-processing function so I can use my own pipeline for different languages such as Chinese and Japanese. And I need to get the internal data structure to be able to visualize it.After some investigations, I decided that summanlp/textrankThe code used in this post has been open-sourced on Github, and comes with a simple web interface:How does TextRank work?We&#x2019;re going to focus on the sentence extraction part of the TextRank algorithm (i.e. ignoring the keyword extraction part). This answer on Quora did a great job explaining the intuition behind the algorithmWhat TextRank does is very simple: it finds how similar each sentence is to all other sentences in the textThe most important sentence is the one that is most similar to all the othersHere are two good tutorials on TextRank algorithm: Document Summarization using TextRankTextRank for Text SummarizationAn ExampleWe&#x2019;re going to use the texts from the first two sections of the &#x201C;Neo-Nazism&#x201D; entry on Wikipedia An static snapshot of the web interface can be found here:The texts were split into 10 sentences, so there are 10 x 9 / 2 = 45 potential undirected connetctions between them. Only 29 of them has a weight larger than 0 (i.e. the sentences at both end of the edges are somewhat similar to each other). The similarity measure used here is the same as the original paper (something very similar to the Jaccard similarity coefficientSimilarity function used. Source: [2]However, the author summanlp/textrankThe Internal GraphThe graph constructed by TextRank (P1 S4 means the fourth sentence in the first paragraph)The above plot actually has some limited interactivity. Please visit the static snapshot page, or clone and run the code from your machine to make use of it.We can see that the first sentence of the seventh paragraph has some strong similarity with P1S1, P1S4 and P5S1 nodes. It implies that the ideas in that sentences had been (partly or fully) mentioned several times in the article, therefore it is most likely the centerpiece of the article.(Note: the paragraph count includes empty lines.)The ResultsThe texts with the most two important sentences highlightedLooks reasonable enough. The highlighted sentences very broadly defines Neo-Nazism. Terms used in them frequently overlap the terms in other sentences.Future WorkIt appears that we already have a decent baseline for Wikipedia-like articles. However, I did not include a proper stop list in the program, so it might work terribly with more casual texts. Using POS tags could actually be a better idea as it is more robust than a fixed stop word list.Bypassing the built-in text pre-processing procedure has not been tried yet. It is required for the program to be able to process other languages that is not supported by SnowballStemmerSyntacticUnitThe EndThank you for reading! As TextRank is a fairly &#x201C;old&#x201D; algorithm by today&#x2019;s standard, I wasn&#x2019;t very sure how much efforts I need to put in to explain the algorithm. In the end I seem to have skipped most of the details.The goal here is to provide a big picture and an example of how everything comes together for someone who is trying to quick build a baseline. I hope you find this post useful and worth your time (please give it some claps if you do). Thanks again for reading!20181212 UpdateI&#x2019;m working on Chinese support, and have uploaded a very early version to the Github repo. You can check this file text_cleaning_zh.py custom pre-processing pipeline with a POS tag filterA snapshot of the Chinese support (WIP)References:Mihalcea, R., Tarau, P.: &#x201C;Textrank: Bringing order into texts&#x201D;Barrios, F., L&#xF3;pez, F., Argerich, L., Wachenchauzer, R.: &#x201C;Variations of the Similarity Function of TextRank for Automated Summarization&#x201D;(This post is also published on my personal blog",
  "image": "https://miro.medium.com/v2/resize:fit:1200/1*0aJebtztCdf2RPbsIEEmEg.jpeg",
  "word_count": 904,
  "keyphrases": []
}