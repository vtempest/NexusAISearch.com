<p align="center">
<img width="200px" src="https://i.imgur.com/4GOOM9s.jpeg"> 
</p>

## ðŸ“œðŸšœ  Tractor the Text Extractor ðŸ“œðŸšœ

* URL to Main Content & Cite - extract main article text with formatted basic markup, using Postlight Parser which improves the Readability algorithm with  site-specific selectors
* Cite - select html tag classes and metadata for author, date, source, and title.
* Youtube to Transcript - via official API and youtubetranscript.com
* PDF to HTML Markup - with text cleaning, joining into paragraphs, infering headings by text height, moving footnotes to end
* Top Sentences and Keyphrases -  Weights sentences using TextRank noun keyphrase frequency  to find which sentences centralize and tie together keyphrase concepts refered to most by other sentences.

## References  

#### Software Library Docs

*   [NLP-Compromise](https://observablehq.com/@spencermountain/nlp-compromise) - Language tokenizer for part-of-speech, split sentences, dictionary, wiki, names. [Video Intro](https://vimeo.com/496095722)

*   [CommonCrawl](https://commoncrawl.org/examples) - crawls and downloads the entire internet 100TB urls and html, and calculates domain rank for 100M domains. Well-funded nonprofit for open source public dataset.

#### Webpage Articles 

* [Automatic Summarization from TextRank to Transformers](https://blog.fastforwardlabs.com/2021/09/22/automatic-summarization-from-textrank-to-transformers.html)

*   [NLP Algorithms Visualized](https://github.com/janlukasschroeder/nlp-cheat-sheet-python/blob/master/README.md)

#### Research Papers


* Kazemi et al (2020). Biased TextRank: Unsupervised Graph-Based Content Extraction. Proceedings of the 28th International Conference on Computational Linguistics. https://aclanthology.org/2020.coling-main.144.pdf

*  Zhao et al (2021). An Improved TextRank Multi-feature Fusion Algorithm For Keyword Extraction of Educational Resources. J. Phys.: Conf. Ser. 2078 012021. https://iopscience.iop.org/article/10.1088/1742-6596/2078/1/012021/pdf


* Huang et al (2024). Compression Represents Intelligence Linearly. https://arxiv.org/html/2404.09937v1  https://doi.org/10.48550/arXiv.2404.09937